{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9326377",
   "metadata": {},
   "source": [
    "## Lab 2 - Applied Machine Learning (DT4031, DT4033) VT26\n",
    "\n",
    "### Instructions\n",
    "##### Flu Shot Learning: Predicting H1N1 and Seasonal Flu Vaccines\n",
    "Vaccination is one of the key measures in global health to reduce spread of infection, diseases and death, as well as decrease both costs and workload in healthcare. One flu known for its annual vaccination is the seasonal flu, caused by Influenza A. This type of flu is particularly persistent and mutating, which makes it necessary to keep developing the existing vaccine in order to give some protection to people at risk. Another known influenza is the H1N1 flu, also known as the swine flu. In 2009, this flu was declared to be a pandemic by the World Health Organization due to its widespread infections and high rates of mortality. In addition to the normal seasonal flu vaccine, a separate vaccine was developed for this particular influenza with improved efficiency of reducing the rates and seriousness of the illness. Millions of doses were given across populations in the world in order to overcome the pandemic. \n",
    "\n",
    "This is a multilabel classification task where your goal is to predict how likely individuals are to receive their H1N1 and seasonal flu vaccines using information they shared about their backgrounds, opinions, and health behaviours. Specifically, you will be predicting two probabilities: one for h1n1_vaccine and one for seasonal_vaccine. This will be done for multiple classifiers, which results you will then compare and analyze. \n",
    "\n",
    "The data comes from the National 2009 H1N1 Flu Survey (NHFS), which was a list-assisted random-digit-dialing telephone survey, designed to monitor influenza immunization coverage in the 2009-10 season. The target population was people older than 6 months in the United States. Each row in the dataset represents one person who responded to the survey. \n",
    "\n",
    "You can read more about the dataset and the challenge this lab is based upon here: https://www.drivendata.org/competitions/66/flu-shot-learning/page/211/  \n",
    "\n",
    "### Submission \n",
    "* Submit all your code as a single Jupyter notebook.\n",
    "* Structured report that includes Introduction, Method, Results, Discussion. The results should contain the evaluation, including figures comparing the different classifiers. The report should NOT contain any code. An analysis should be performed on the results, comparing the different classifiers to each other and the effect of optimizing some hyperparameters. The discussion should additionally contain contemplations guided by the questions in the lab, regarding data exploration, method choices, effects of feature selections, and evaluation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ce7e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install missingno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cdca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import missingno as msno \n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a343a8",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b7f45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.read_csv(\"features.csv\")\n",
    "df_labels = pd.read_csv(\"labels.csv\")\n",
    "df = pd.concat([df_features, df_labels], axis = 1)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5829915a",
   "metadata": {},
   "source": [
    "#### Data description \n",
    "Make sure you spend some time understanding the data. Answer the following questions (for example using figures): \n",
    "* In the data, how many people took the H1N1 vaccine vs the seasonal vaccine? How many didn't take it? \n",
    "* Which columns hold categorical data and which hold numerical data (including binary)? \n",
    "* Is there an imbalance in the data, considering the labels? \n",
    "* Choose 3 columns and check if they have any boundary violations by comparing the values to the description of what values it should contain (given in the link above). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd82c979",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b7b994",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add code here to explore the dataset and answer the given questions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be36e9a0",
   "metadata": {},
   "source": [
    "#### Some quick preprocessing\n",
    "When we look at df.describe() we can see that the count between the different features vary, which indicates that there are missing values we need to take care of. Let's investigate.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21c392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3a57a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9deb15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values_df = df.isna().sum()\n",
    "\n",
    "features = []\n",
    "missing = []\n",
    "\n",
    "for x in missing_values_df:\n",
    "    missing.append((x / df.shape[0]) * 100) \n",
    "    \n",
    "for feature in df:\n",
    "    features.append(feature)\n",
    "    \n",
    "data = {'Features': features, 'Missing': missing}\n",
    "\n",
    "missing_values = pd.DataFrame(data)\n",
    "missing_values = missing_values.sort_values(by=[\"Missing\"], ascending=[False])\n",
    "sns.set(rc={'figure.figsize':(6,10)})\n",
    "missing_plot = sns.barplot(y=\"Features\", x ='Missing', data=missing_values)\n",
    "\n",
    "plt.xlabel(\"Missing values (%)\", fontsize=20)\n",
    "plt.ylabel(\"Features\", fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cb4572",
   "metadata": {},
   "source": [
    "Looking at the information above, decide which features have a very high amount of missing values. What potential problem could arise from including these features in the dataset during model training? Consider the impact of removing these based on your reasoning, decide which features are most appropriate to exclude. This could be none, some, or all of them.\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87c4731",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[]) #add the chosen features in the brackets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702e5260",
   "metadata": {},
   "source": [
    "Now we fix the remaining missing values. We will impute numerical and categorical features seperately. For categorical features we will for simplicity use the most frequent values to impute with. \n",
    "\n",
    "Before we impute we need to split our dataset. This is due to not wanting to leak any data while imputing. If we imputed with the test data, the training data could be influenced and the test data would thereby not be unseen. Therefore, we will fit the imputer to the training data and use it to transform our test and validation dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7952d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df.drop(columns=[\"h1n1_vaccine\", \"seasonal_vaccine\"])\n",
    "y = df[[\"h1n1_vaccine\", \"seasonal_vaccine\"]]\n",
    "\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=, random_state=42) #add size of split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=, random_state=42) #add size of split\n",
    "\n",
    "\n",
    "\n",
    "print(\"Missing values in X_train:\", X_train.isna().sum().sum())\n",
    "print(\"Missing values in X_validation:\", X_val.isna().sum().sum()) \n",
    "print(\"Missing values in X_test:\", X_test.isna().sum().sum()) \n",
    "print(\"Missing values in y_train:\", y_train.isna().sum().sum()) #should already be 0\n",
    "print(\"Missing values in y_validation:\", y_val.isna().sum().sum()) #should already be 0\n",
    "print(\"Missing values in y_test:\", y_test.isna().sum().sum()) #should already be 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433d8289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "num_cols = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns #numerical columns\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\"]).columns #categorical columns\n",
    "\n",
    "#numerical\n",
    "num_imputer = IterativeImputer(random_state=42)\n",
    "X_train[num_cols] = num_imputer.fit_transform(X_train[num_cols])\n",
    "X_val[num_cols] = num_imputer.transform(X_val[num_cols])\n",
    "X_test[num_cols] = num_imputer.transform(X_test[num_cols])\n",
    "\n",
    "#categorical\n",
    "cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "X_train[cat_cols] = cat_imputer.fit_transform(X_train[cat_cols])\n",
    "X_val[cat_cols] = cat_imputer.transform(X_val[cat_cols])\n",
    "X_test[cat_cols] = cat_imputer.transform(X_test[cat_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5221a2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#these should be 0 if imputed correctly\n",
    "print(\"Missing values in X_train:\", X_train.isna().sum().sum())\n",
    "print(\"Missing values in X_validation:\", X_val.isna().sum().sum()) \n",
    "print(\"Missing values in X_test:\", X_test.isna().sum().sum()) \n",
    "print(\"Missing values in y_train:\", y_train.isna().sum().sum())\n",
    "print(\"Missing values in y_validation:\", y_val.isna().sum().sum()) \n",
    "print(\"Missing values in y_test:\", y_test.isna().sum().sum()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac397b6",
   "metadata": {},
   "source": [
    "#### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81917387",
   "metadata": {},
   "source": [
    "When it comes to working with real-world datasets, they are often messy and can contain hundreds or even thousands of features for every instance. This can become problematic as high dimensionality can negatively affect model performance (see the curse of dimensionality) while many features may contribute little to the model's decision-making process. \n",
    "\n",
    "Feature selection is a data pre-processing method used to reduce the number of features in a dataset by identifying redundant and irrelevant features. This can help the model to, among other things, reduce the risk of overfitting, decrease computational requirements and facilitate interpretability of results. \n",
    "\n",
    "There are multiple approaches to feature selection, ranging from automated algorithms to conceptual methods based on common sense, domain knowledge and data-quality considerations. You already did some feature selection when handling features with a lot of missing values. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7162fa0",
   "metadata": {},
   "source": [
    "Consider all features in the dataset in the context of the task: a multilabel classification of H1N1 and sesonal flu vaccination. Are there any features that could give one label more direct information than the other? How might this affect model learning, and which features, if any, might you consider removing to ensure the model receives comparable information for both labels? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9ceb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a794e5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cols_to_drop = [] #add potential features that you wish to drop based on your reasoning\n",
    "\n",
    "X_train = X_train.drop(columns=cols_to_drop)\n",
    "X_val = X_val.drop(columns=cols_to_drop)\n",
    "X_test = X_test.drop(columns=cols_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08db0055",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "print(y_train.shape, y_val.shape, y_test.shape) #make sure it matches "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a759e9eb",
   "metadata": {},
   "source": [
    "Now that we have relevant candidates for features, we will try an algorithm feature selection technique called SelectKBest. SelectKBest score each feature independently against the target variable using a statistical test, and then selects the top K features with the highest scores. This means that the features are based on the scoring function. \n",
    "Read more about it here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html. \n",
    "\n",
    "As the task is multilabel, we will use the f_classif scoring function seperately for each target variable. For each feature, we will calculate its score for both H1N1 and seasonal vaccines, and then compute an average score to identify features that are important for both targets. This requires numerical values, so first we will be encoding these. Please note that using the average, we are considering which features are the most important for both targets at once. If we want to achieve better performance for the task, getting the most important features for each target would probably yield better results. However, for simplicity, we are considering both at the same time. If you are interested and feel confident in changing more of the code, you can try making separate models and feature selections for the different targets, but this is not a requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c12cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "num_cols = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns #numerical columns\n",
    "cat_cols = X_train.select_dtypes(include=[\"object\"]).columns #categorical columns\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "X_train_encoded = encoder.fit_transform(X_train[cat_cols])\n",
    "X_val_encoded = encoder.transform(X_val[cat_cols])\n",
    "X_test_encoded = encoder.transform(X_test[cat_cols])\n",
    "\n",
    "X_train_cat = pd.DataFrame(X_train_encoded, columns=encoder.get_feature_names_out(cat_cols)).reset_index(drop=True)\n",
    "X_val_cat = pd.DataFrame(X_val_encoded, columns=encoder.get_feature_names_out(cat_cols)).reset_index(drop=True)\n",
    "X_test_cat = pd.DataFrame(X_test_encoded, columns=encoder.get_feature_names_out(cat_cols)).reset_index(drop=True)\n",
    "\n",
    "X_train = pd.concat([X_train[num_cols].reset_index(drop=True), X_train_cat], axis=1)\n",
    "X_val= pd.concat([X_val[num_cols].reset_index(drop=True), X_val_cat], axis=1)\n",
    "X_test = pd.concat([X_test[num_cols].reset_index(drop=True), X_test_cat], axis=1)\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape) #the amount of columns should now have increased\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad8420a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "selector_h1n1 = SelectKBest(score_func=f_classif, k='all')\n",
    "selector_h1n1.fit(X_train, y_train[\"h1n1_vaccine\"])\n",
    "h1n1_scores = pd.DataFrame({\"Feature\": X_train.columns,\"F_score_h1n1\": selector_h1n1.scores_})\n",
    "\n",
    "selector_seasonal = SelectKBest(score_func=f_classif, k='all')\n",
    "selector_seasonal.fit(X_train, y_train[\"seasonal_vaccine\"])\n",
    "seasonal_scores = pd.DataFrame({\"Feature\": X_train.columns,\"F_score_seasonal\": selector_seasonal.scores_})\n",
    "\n",
    "feature_scores = pd.merge(h1n1_scores, seasonal_scores, on=\"Feature\")\n",
    "feature_scores[\"Average_score\"] = feature_scores[[\"F_score_h1n1\",\"F_score_seasonal\"]].mean(axis=1)\n",
    "\n",
    "feature_scores = feature_scores.sort_values(by=\"Average_score\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "feature_scores.head(10) #change here to see more or less features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3702a1e",
   "metadata": {},
   "source": [
    "Looking at the feature scores, do you think they make sense? Why/why not? Then choose a k below. As this is multilabel, we will choose the features based on their average scores. Normally you could use the transform method for this (non-multilabel), but here we do it a bit more manual. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2009e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = #the number of features you want to keep based on average score\n",
    "top_features = feature_scores[\"Feature\"].head(top_k).tolist()\n",
    "print(\"Top features by average score:\", list(top_features))\n",
    "\n",
    "X_train = X_train[top_features]\n",
    "X_val = X_val[top_features]\n",
    "X_test = X_test[top_features]\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape) #should match your chosen features "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca52113",
   "metadata": {},
   "source": [
    "### Part 2 \n",
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e1b67a",
   "metadata": {},
   "source": [
    "Finally training time! We've implemented two models for you below. The code uses a MultiOutputClassifier (https://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) that fits one classifier per target. If you want to, you can instead train one model for each target. Model number 1 is a decision tree: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbba525",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "base_tree = DecisionTreeClassifier(random_state=42)\n",
    "multi_tree = MultiOutputClassifier(base_tree)\n",
    "\n",
    "multi_tree.fit(X_train, y_train)\n",
    "\n",
    "y_val_pred = multi_tree.predict(X_val)\n",
    "\n",
    "print(\"Accuracy (val): \", accuracy_score(y_val, y_val_pred))\n",
    "\n",
    "print(\"Validation Metrics\")\n",
    "print(classification_report(y_val, y_val_pred, target_names=[\"H1N1\", \"Seasonal\"], zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b23cff",
   "metadata": {},
   "source": [
    "Most often models don't perform well immediately but require some hyperparameter optimization to reach a more satisfactory result. You can check the individual models parameters to check what you can try to optimize. For the decision tree, the depth of tree can be crucial. Try picking a suitable range of depth for better performance. \n",
    "\n",
    "Note that since we use a MultiOutputClassifier, the accuracy shows over both target variables. This means we will hence optimize for both target variables at once, and only get one set of hyperparameters. Most likely, this is not the most optimal hyperparameters for the target variables individually. If you want, you can change this and train two models individually, and in extension probably receive better performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d8f77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = []\n",
    "accuracies = []\n",
    "for i in range(): # pick a suitable range\n",
    "    decision_tree_base = DecisionTreeClassifier(max_depth=i)\n",
    "    decision_tree_model = MultiOutputClassifier(decision_tree_base)\n",
    "    decision_tree_model.fit(X_train, y_train)\n",
    "    decision_tree_pred = decision_tree_model.predict(X_val)\n",
    "    accuracy = round(accuracy_score(y_val, decision_tree_pred), 5)\n",
    "    max_depth.append(i)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "decision_tree_hyperparameter_tuning_df = pd.DataFrame({'max_depth': max_depth, 'accuracy': accuracies})\n",
    "decision_tree_hyperparameter_tuning_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc00782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose your optimal hyperparameters for a final model \n",
    "decision_tree = DecisionTreeClassifier(max_depth=) \n",
    "optimal_decision_tree_model = MultiOutputClassifier(decision_tree) \n",
    "optimal_decision_tree_model.fit(X_train, y_train)\n",
    "optimal_decision_tree_pred = optimal_decision_tree_model.predict(X_test) #here we predict the hold-out dataset\n",
    "optimal_decision_tree_pred_prob = optimal_decision_tree_model.predict_proba(X_test)\n",
    "optimal_decision_tree_y_preds = pd.DataFrame({\"h1n1_vaccine\": optimal_decision_tree_pred_prob[0][:, 1],\"seasonal_vaccine\": optimal_decision_tree_pred_prob[1][:, 1],},)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390675fb",
   "metadata": {},
   "source": [
    "Model number two is an ensemble learning model called AdaBoost. Read more about it here: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b96ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "basic_ada_boost_model = AdaBoostClassifier(algorithm=\"SAMME\")\n",
    "basic_ada_boost_multi_model= MultiOutputClassifier(basic_ada_boost_model)\n",
    "basic_ada_boost_multi_model=basic_ada_boost_multi_model.fit(X_train, y_train)\n",
    "basic_ada_boost_y_pred = basic_ada_boost_multi_model.predict(X_val)\n",
    "\n",
    "print(\"Accuracy (val): \", accuracy_score(y_val, basic_ada_boost_y_pred))\n",
    "\n",
    "print(\"Validation Metrics\")\n",
    "print(classification_report(y_val, basic_ada_boost_y_pred, target_names=[\"H1N1\", \"Seasonal\"], zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47becf34",
   "metadata": {},
   "source": [
    "Similarly, we will try some basic optimization here. Choose which numbers of estimators you'd like to try and choose the one that works the best. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15c4177",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies_test = []\n",
    "accuracies_train = []\n",
    "\n",
    "n_estimators = [] #choose which values to try \n",
    "\n",
    "for est in n_estimators:     \n",
    "    ada_boost_model = AdaBoostClassifier(n_estimators=est, algorithm=\"SAMME\")\n",
    "    ada_boost_multi_model= MultiOutputClassifier(ada_boost_model)\n",
    "    ada_boost_multi_model=ada_boost_multi_model.fit(X_train, y_train)\n",
    "    ada_boost_y_pred = ada_boost_multi_model.predict(X_val)\n",
    "    ada_boost_y_train_pred = ada_boost_multi_model.predict(X_train)\n",
    "    accuracy_test = accuracy_score(y_val, ada_boost_y_pred)\n",
    "    accuracy_train = accuracy_score(y_train, ada_boost_y_train_pred)\n",
    "    accuracies_test.append(accuracy_test)\n",
    "    accuracies_train.append(accuracy_train)\n",
    "    \n",
    "\n",
    "ada_hpo = pd.DataFrame({'n_estimators': n_estimators, 'Accuracy train': accuracies_train, 'Accuracy val': accuracies_test})\n",
    "ada_hpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddd8d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_nestimators =  #choose your value from last cell\n",
    "optimal_ada_boost_model = AdaBoostClassifier(n_estimators = optimal_nestimators, algorithm = \"SAMME\")\n",
    "optimal_ada_boost_multi_model= MultiOutputClassifier(optimal_ada_boost_model)\n",
    "optimal_ada_boost_multi_model=optimal_ada_boost_multi_model.fit(X_train, y_train)\n",
    "optimal_ada_boost_y_pred = optimal_ada_boost_multi_model.predict(X_test) #predict hold-out data\n",
    "optimal_ada_boost_pred_prob = optimal_ada_boost_multi_model.predict_proba(X_test)\n",
    "optimal_ada_boost_y_preds = pd.DataFrame(\n",
    "    {\n",
    "        \"h1n1_vaccine\": optimal_ada_boost_pred_prob[0][:, 1],\n",
    "        \"seasonal_vaccine\": optimal_ada_boost_pred_prob[1][:, 1],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4920fee",
   "metadata": {},
   "source": [
    "Now it's your turn! Choose a model and make some hyperparameter optimization and prepare it for comparison. You can find more classifiers for example from lectures, in Chapter 7 from \"Hands-On Machine Learning with Scikit-Learn, Keras, and Tensorflow\", or in the documentation from scikit-learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4423385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bf6ac5",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fac4a3",
   "metadata": {},
   "source": [
    "Finally, we will compare the models based on hold-out data and look on how to evaluate the models. So far we've created optimal models and predicted on the hold-out dataset using those. Let's look at the metrics. When analyzing, consider what metrics are the most useful in the context of the task. Can the importance of different metrics differ between different tasks? Give an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de459fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,f1_score,roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6287a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_multilabel_model(model_name,y_true,y_pred,y_pred_proba,label_names=(\"h1n1_vaccine\", \"seasonal_vaccine\"),):\n",
    "    metrics = {\n",
    "        \"model\": model_name,\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision_macro\": precision_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
    "        \"recall_macro\": recall_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
    "        \"f1_macro\": f1_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
    "    }\n",
    "\n",
    "    y_proba = pd.DataFrame(\n",
    "        {\n",
    "            label: y_pred_proba[i][:, 1]\n",
    "            for i, label in enumerate(label_names)\n",
    "        }).values\n",
    "\n",
    "    metrics[\"roc_auc_macro\"] = roc_auc_score(y_true,y_proba,average=\"macro\",multi_class=\"ovo\")\n",
    "\n",
    "    return pd.DataFrame([metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6b74ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "results.append(\n",
    "    evaluate_multilabel_model(\n",
    "        model_name=\"Decision Tree\",\n",
    "        y_true=y_test,\n",
    "        y_pred=optimal_decision_tree_pred,\n",
    "        y_pred_proba=optimal_decision_tree_pred_prob,\n",
    "    )\n",
    ")\n",
    "\n",
    "results.append(\n",
    "    evaluate_multilabel_model(\n",
    "        model_name=\"AdaBoost\",\n",
    "        y_true=y_test,\n",
    "        y_pred=optimal_ada_boost_y_pred,\n",
    "        y_pred_proba=optimal_ada_boost_pred_prob,\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "#add metrics for your own model here \n",
    "\n",
    "metrics_df = pd.concat(results, ignore_index=True)\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dea6a3",
   "metadata": {},
   "source": [
    "If we want to visualise this more clearly, we can look at the confusion matrices. This will show us what the model predicts in comparison to their true labels in a classification task. We detect true positives(TP, model predicted correct positive class), true negatives (TN, model predicted correct negative class), false positives(FP, model predicts a positive class when in reality it's not), and false negatives (model predicts a negative class when it should be positive). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89d1af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "\n",
    "def plot_multilabel_confusion_matrices(model_cm_dict, labels=(\"H1N1\", \"Seasonal\"), cmap=\"Blues\"):\n",
    "    n_models = len(model_cm_dict)\n",
    "    n_rows = n_models\n",
    "    n_cols = len(labels)\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(6 * n_cols, 4 * n_rows))\n",
    "\n",
    "    if n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "\n",
    "    for i, (model_name, cms) in enumerate(model_cm_dict.items()):\n",
    "        for j, cm in enumerate(cms):\n",
    "            ax = axes[i, j]\n",
    "            sns.heatmap(cm, annot=True, fmt=\"d\", cmap=cmap, ax=ax)\n",
    "            ax.set_title(f\"{model_name} - {labels[j]}\", fontsize=14)\n",
    "            ax.set_xlabel(\"Predicted value\", fontsize=10)\n",
    "            ax.set_ylabel(\"True value\", fontsize=10)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "optimal_decision_tree_cm = multilabel_confusion_matrix(\n",
    "    y_true=y_test,\n",
    "    y_pred=optimal_decision_tree_pred\n",
    ")\n",
    "\n",
    "optimal_adaboost_cm = multilabel_confusion_matrix(\n",
    "    y_true=y_test,\n",
    "    y_pred=optimal_ada_boost_y_pred\n",
    ")\n",
    "\n",
    "#add code here to add your own model \n",
    "\n",
    "\n",
    "\n",
    "model_cm_dict = {\n",
    "    \"Optimal DT\": optimal_decision_tree_cm,\n",
    "    \"Optimal ADA\": optimal_adaboost_cm\n",
    "}\n",
    "\n",
    "plot_multilabel_confusion_matrices(model_cm_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff996d91",
   "metadata": {},
   "source": [
    "A useful evaluation is looking at the Reciever Operating Characteristic (ROC)-curve. This shows how well a model's predicted probabilities separate positive from negative cases across all thresholds, with the area under the curve (AUC) summarizing this ability as a single number. 0.5 is considered to be random, while 1.0 is a perfect (with curve in the upper left corner). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db830dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "def plot_single_roc(y_true, y_score, label):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    auc = roc_auc_score(y_true, y_score)\n",
    "    plt.plot(fpr, tpr, label=f\"{label} (AUC = {auc:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07df94e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#roc curve for H1N1 \n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "\n",
    "plot_single_roc(\n",
    "    y_test[\"h1n1_vaccine\"],\n",
    "    optimal_decision_tree_y_preds[\"h1n1_vaccine\"],\n",
    "    label=\"Decision Tree\"\n",
    ")\n",
    "\n",
    "plot_single_roc(\n",
    "    y_test[\"h1n1_vaccine\"],\n",
    "    optimal_ada_boost_y_preds[\"h1n1_vaccine\"],\n",
    "    label=\"AdaBoost\"\n",
    ")\n",
    "\n",
    "\n",
    "#add code here to add your own model \n",
    "\n",
    "# chance line\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"grey\", linewidth=2)\n",
    "\n",
    "plt.xlabel(\"False Positive Rate (FPR)\", fontsize=14)\n",
    "plt.ylabel(\"True Positive Rate (TPR)\", fontsize=14)\n",
    "plt.title(\"a) ROC Curve for H1N1 Vaccine\", fontsize=16)\n",
    "\n",
    "plt.xticks(np.arange(0.0, 1.1, 0.1))\n",
    "plt.yticks(np.arange(0.0, 1.1, 0.1))\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db3beec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add code here to plot a ROC curve for the seasonal vaccine for all tested models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ab6890",
   "metadata": {},
   "source": [
    "Reason around the confusion matrices and the ROC-curves for the two different labels and compare the models. \n",
    "\n",
    "* Which of the tested classification models is the most suitable to predict vaccination rate of the H1N1 and the Seasonal vaccine?\n",
    "* Does one vaccine seem easier to predict than the other?\n",
    "* If you were to continue the work to get a better performance, what could you do differently or add? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d8a8dd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
